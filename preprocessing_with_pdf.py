# -*- coding: utf-8 -*-
"""Model_Integration.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1_MezF6llFMlhZ-ibc32Na-KwwON39OD4
Importing Libraries
"""

#!pip install emot
!pip install pyLDAvis

import csv
import string
import numpy as numpy
import pandas as pd
import os
from collections import Counter
from textblob import TextBlob
import math
from sklearn.metrics import classification_report
import re
import pickle
import string
import nltk
from emot.emo_unicode import UNICODE_EMO, EMOTICONS
import spacy
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from sklearn.metrics.pairwise import cosine_similarity
from numpy import array
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
import pyLDAvis
import pyLDAvis.gensim 
import matplotlib.pyplot as plt
import random

"""Function Definitions"""

def get_sentiment(text):
    analysis = TextBlob(text) 
    polarity= analysis.sentiment.polarity
    subjectivity= analysis.subjectivity
    
    return polarity, subjectivity
#Syllable Counter
def count_syllables(word):
    return len(
        re.findall('(?!e$)[aeiouy]+', word, re.I) +
        re.findall('^[^aeiouy]*e$', word, re.I)
    )
#Content Analyser
def content_analysis(text):
    num_words= len(text.split())
    temp= Counter(text.split())
    diff_words= set()
    x=0 
    syll= 0
    for word in temp:
        x= x+temp[word]*(math.log(num_words)- math.log(temp[word]))
        syll= count_syllables(word)        
        if syll >= 2: 
            diff_words.add(word) 

    complex_words= len(diff_words)
    Readability= 0.4*(num_words+ 100*(complex_words/num_words))    
    Complexity= (1/num_words)*x
    return Complexity, Readability, num_words

"""TF IDF Vectorization for N Grams"""

def tf_idf(text):
    #Add paths to pickle files here
    tf_file= open("vect_file", 'rb')
    Vocab_file=open("lemma_file", 'rb') 
    
    #Loading vecotrizer and vocabulary
    vect= pickle.load(tf_file)
    vocab= pickle.load(Vocab_file)
    temp= vect.fit_transform(vocab)
    
    text= [text]
    v= vect.transform(text)
    d= v.todense()
    return d

"""Text Cleaning"""

nltk.download('wordnet')

STOPWORDS = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV} # Pos tag, used Noun, Verb, Adjective and Adverb

#function to transform abbreviations from slang_dict
def translator(user_string):
    user_string = user_string.split(" ")
    j = 0
    for _str in user_string:
        # File path which consists of Abbreviations.
        fileName = "slang_dict_edited_more.txt"
        # File Access mode [Read Mode]
        accessMode = "r"
        with open(fileName, accessMode) as myCSVfile:
            # Reading file as CSV with delimiter as "=", so that abbreviation are stored in row[0] and phrases in row[1]
            dataFromFile = csv.reader(myCSVfile, delimiter="`")
            # Removing Special Characters.
           # _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)
            for row in dataFromFile:
                # Check if selected word matches short forms[LHS] in text file.
                if _str.upper() == row[0]:
                    # If match found replace it with its appropriate phrase in text file.
                    user_string[j] = row[1]
            myCSVfile.close()
        j = j + 1
    # Replacing commas with spaces for final output.
    return (' '.join(user_string))

def convert_emojis(text):
    for emot in UNICODE_EMO:
        text = text.replace(emot, "_".join(UNICODE_EMO[emot].replace(",","").replace(":","").split()))
    return text
def convert_emoticons(text):
    for emot in EMOTICONS:
        text = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), text)
    return text

def stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])

def lemmatize_words(text):
    pos_tagged_text = nltk.pos_tag(text.split())
    return " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])


#Creating function for tokenization
def tokenization(text):
    text = re.split('\W+', text)
    return text

def text_clean(message):
    message=translator(message)  #abbreviations replaced 
    message= convert_emoticons(message)
    message= convert_emojis(message)
    #lowercasing
    message=message.lower() 
    message = message.replace('[^\w\s]','')  
    message=stopwords(message) 
    message= TextBlob(message)
    message=lemmatize_words(message)
    # message=tokenization(message)
    return message

"""Import pickle files"""

n_gram_file = open('LR_file2', 'rb')      
n_gram_model = pickle.load(n_gram_file)

sentiment_file = open('sentiment_model', 'rb')      
sentiment_model = pickle.load(sentiment_file)

"""TOPIC MODELING"""

nltk.download('punkt')

#CHANGE PATH ACCORDINGLY
path= "Stages_re.csv"
data = pd.read_csv(path, error_bad_lines=False)

data['tokens'] = data.apply(lambda row: nltk.word_tokenize(str(row['text_lemma'])), axis=1)
dictionary = gensim.corpora.Dictionary(data['tokens'])
#print(dictionary)
count = 0
for k, v in dictionary.iteritems():
    #print(k, v)
    count += 1
    # if count > 10:
    #     break
dictionary.filter_extremes(no_below=2, no_above=0.5, keep_n=100000)
bow_corpus = [dictionary.doc2bow(doc) for doc in data['tokens']]

from gensim import corpora, models
tfidf = models.TfidfModel(bow_corpus)
corpus_tfidf = tfidf[bow_corpus]
# from pprint import pprint
# print([[(dictionary[id], freq) for id, freq in cp] for cp in corpus_tfidf[:2819]])
# for doc in corpus_tfidf:
#     pprint(doc)
#     break
# print(len(dictionary))
lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=12, id2word=dictionary, passes=2, workers=2)

docs=[]

c=0
for c in range(0,len(bow_corpus)):
    counter=0
    vector=[]
    for index, score in lda_model[bow_corpus[c]]:
        # print("\nScore: {}\t \nTopic: {}".format(score, index))
        while index!=counter:
            vector.append(0.0)
            counter+=1
        vector.append(score)
        counter+=1
    #number of topics is 8

        
      
    # while(len(vector)!=8):
    #     vector.append(0)
    while counter!=12:
        vector.append(0.0)
        counter+=1   
    c=c+1
    docs.append(vector)
# print(len(bow_corpus)-1)
# print(len(data['question']))
data['msgtopics']=docs
# print(data['msgtopics'])

def find_topic_vector(stagetest):
    test=[]
    # vector1=[]
    # vector2=[]
    # vector3=[]

    # data['tokens'] = data.apply(lambda row: nltk.word_tokenize(str(row['text_lemma'])), axis=1)
    # stage0_string="think bull run hello hi beth make favorite pasta"
    # stagetest=data['text_lemma'][:4].tolist()#comment this
 

    # test_text=" "

  
    # test_text=test_text.join(stagetest)
    
    tokens_stage_test=nltk.word_tokenize(stagetest)
    # tokens_stage1=nltk.word_tokenize(text1)
    # tokens_stage2=nltk.word_tokenize(text2)
    # tokens_stage3=nltk.word_tokenize(text3)

    # print(tokens_stage_test)

    stagetest_string=dictionary.doc2bow(tokens_stage_test)
    # stage1_string=dictionary.doc2bow(tokens_stage1)
    # stage2_string=dictionary.doc2bow(tokens_stage2)
    # stage3_string=dictionary.doc2bow(tokens_stage3)
    # print(stagetest_string)
    # bow_corpus = dictionary.doc2bow(doc) for doc in data['tokens']
    counter=0
    for index, score in lda_model[stagetest_string]:
        # print("\nScore: {}\t \nTopic: {}".format(score, index))
        while index!=counter:
            test.append(0.0)
            counter+=1
        test.append(score)
        counter+=1
        #number of topics is 10
    while counter!=12:
        test.append(0.0)
        counter+=1   

    return test

import numpy as np

def calc_cosine(a,b):
    dot = np.dot(a, b)
    norma = np.linalg.norm(a)
    normb = np.linalg.norm(b)
    cos = dot / (norma * normb)
    return(cos)

def find_label(msg):
    
    msg=text_clean(msg)
    test=find_topic_vector(msg)
    # print(test)
    #pass a text through preprocessing module before testing it
    knn_test=[]
    for index, row in data.iterrows():
        knn_test.append(calc_cosine(data['msgtopics'][index],test ))
    
    

    max3_ind=find_max(knn_test)
    max3_lab=[]
    for j in max3_ind:
        max3_lab.append(data.iloc[j]['Stage'])
    
    # print(max3_lab)

    counter = 0
    num = max3_lab[0] 
      
    for i in max3_lab: 
        curr_frequency = max3_lab.count(i) 
        if(curr_frequency> counter): 
            counter = curr_frequency 
            num = i 
  
    # print(num)
    
    #print( sorted( [(x,i) for (i,x) in enumerate(knn_test)], reverse=True )[:3] )
    # print(knn_test)
#     max_cos= max(calc_cosine(vector0,test),calc_cosine(vector1,test),calc_cosine(vector2,test),calc_cosine(vector3,test))

#     if (max_cos== calc_cosine(vector2,test)):
#         label=2
#     elif (max_cos== calc_cosine(vector1,test)):
#         label=1
#     elif (max_cos== calc_cosine(vector3,test)):
#         label=3
#     else:
#         label=0
    return num

def find_max(list1):
    final_list = [] 
  
    for i in range(0, 3):  
        max1 = 0
        max=0
        for j in range(len(list1)):      
            if list1[j] > max:
                max= list1[j]; 
                max1 = j; 
                
        list1.remove(max); 
        final_list.append(max1) 
        
    return (final_list)

"""Assignment of weights to each class"""

w= dict()
w[0]= 0.0
w[1]= 0.224
w[2]= 0.274
w[3]= 0.50

"""Testing User Input"""

def test_fin(test): 
    final_label= 0
    pred_list= list()

    test_msg= test
    test= text_clean(test)
    #print(test)
    test_data= {"Sentiment":[], "Subjectivity":[], "Complexity":[], "Readability":[], "Length":[]}
    score, context= get_sentiment(test)
    C, R, num_words= content_analysis(test)
    test_data["Sentiment"].append(score)
    test_data["Subjectivity"].append(context)
    test_data["Complexity"].append(C)
    test_data["Readability"].append(R)
    test_data["Length"].append(num_words)

    test_data= pd.DataFrame(test_data)
    mat= tf_idf(test)

    pred_ngrams= int(n_gram_model.predict(mat))
    # #     print(pred_ngrams)

    predx= int(sentiment_model.predict(test_data)) 
    # #     print(predx)

    stage_lab= int(find_label(test_msg))
    #     print(stage_lab)

    pred_list.append(pred_ngrams)
    pred_list.append(predx)
    pred_list.append(stage_lab)

    print(pred_list)

    if (all(v == 0 for p in pred_list)):
        final_label= 0
    elif (len(set(pred_list)) == len(pred_list)):
        final_label= 2
    else:
        result= max(set(pred_list), key = pred_list.count) 
        if result==0:
            for x in pred_list:
                if x!=0:
                    final_label=x

        else:
            final_label= result

    return final_label

"""Taking user Input Repeatedly"""

nltk.download('averaged_perceptron_tagger')

test= str(input("Enter test string:\n"))
final_label= test_fin(test)
print("Final Label for "+test+" :")
print(final_label)

weight= w[final_label]
print(weight)

def txt_to_csv(textfile):
  filename=textfile
  df=pd.read_csv(filename,header=None,error_bad_lines=False)
  # df.head()
  df = df.iloc[1:]
  print(df.head())
  #creating desired columns for csv
  df['Chat']=""
  df.columns=['Name', 'Chat']
  print(df.head())
  Message= df['Name'].str.split('\\):', n = 1, expand = True)
  print(Message.head())
  Message.columns=['Name_Time', 'Chat']
  Message1= Message['Name_Time'].str.split('\\(', n = 1, expand = True)
  print(Message1.head())
  result = pd.concat([Message1, Message], axis=1, sort=False)
  print(result.head())
  result= result.drop(['Name_Time'], axis = 1) 
  # result.head()
  for i in result.index:
    x=re.search('[(](.*)[)]',str(result["Chat"][i]))
    if x is not None:
      result["Chat"][i] = result["Chat"][i].replace("("+str(x.group(1))+")","")
  # result.head() 
  
  
  for i in range(1,len(result)) : 
    string= result.loc[i, 'Chat']
    if(string is None):
      continue
    result.loc[i, 'Chat']=translator(result.loc[i, 'Chat'])  #abbreviations replaced 

  
  result['Chat'] = result['Chat'].fillna("").apply(convert_emoticons) #emoticons and emojis replaced 
  result['Chat'] = result['Chat'].fillna("").apply(convert_emojis)
  #lowercasing
  result['text_lower']  = result['Chat'].str.lower()
  result['text_lower'].head()
  result.head()
  result['text_punct'] = result['text_lower'].str.replace('[^\w\s]','')
  result['text_punct'].head()
  result.head()
  result["text_stop"] = result["text_punct"].apply(stopwords)
  result["text_stop"].head()
  print(result)
  
  if(string is None):
    result['text_stop'].apply(lambda x: str(TextBlob(x).correct()) if (len(x)!=0) else (x))

  result["text_lemma"] = result["text_stop"].apply(lemmatize_words)
  result['text_token'] = result['text_lemma'].apply(lambda x: tokenization(x.lower()))
  result[['text_token']].head()
  result.head(50)
  return result

#!pip install fpdf


from fpdf import FPDF 


# save FPDF() class into a 
# variable pdf 
pdf = FPDF() 

# Add a page 
pdf.add_page() 

# set style and size of font 
# that you want in the pdf 
pdf.set_font("Arial", size = 11) 
pdf.cell(200, 10, txt = "Evidence Report",  
         ln = 1, align = 'C')

pdf.ln(5)

pdf.rect(5.0, 5.0, 200.0,287.0)

weight1=0.224
weight2=0.274
weight3= 0.5

def uploaded_file(filename):
  test_frame= txt_to_csv(filename)
  test_frame['training_label']=0
  score=0
  counter=0
  count1=0
  count2=0
  count3=0
  score1=0
  score2=0
  score3=0
  for index, row in test_frame.iterrows():
    # print(row['c1'], row['c2'])
    counter+=1
    label=find_label(str(row["text_lemma"]))
    test_frame['training_label'][index]=label
    if label==1:
      count1+=1
      score+= weight1
      score1+=weight1
    elif label==2:
      count2+=1
      score+=weight2
      score2+=weight2
    elif label==3:
      count3+=1
      score+= weight3
      score3+=weight3
    if (score>0.09 and (label==3 or label==2)):
      pdf.cell(200, 10, txt = row["Chat"],  
         ln = 1, align = 'L')
  threshold=0.15
  av2=score2/score
  av3=score3/score
  risk_list=[]
  list_av=[]
  #list_av.append(av1)
  list_av.append(av2)
  list_av.append(av3)
  list_av2=[]
  for i in list_av:
      list_av2.append(i-threshold)
  for i in range(0,len(list_av2)):
      if(list_av2[i]>0):
          risk_list.append(list_av2[i]/list_av[i])

  if(len(risk_list)>1):
      max_risk=risk_list[0]
      for i in range(0,len(risk_list)):
        if(risk_list[i]>max_risk):
            max_risk=risk_list[i]
            index=i+2
  else:
      max_risk=risk_list[0]
      index=3
  print("The risk factor is %s for the level %s"%((max_risk*100),index))



filename="quin_says.txt" 
uploaded_file(filename)

pdf.output("evidence7.pdf")
